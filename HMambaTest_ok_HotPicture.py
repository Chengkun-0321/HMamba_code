import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
import numpy as np
from dataclasses import dataclass
from matplotlib import pyplot as plt
import os
import argparse
import csv
import seaborn as sns
from tqdm import tqdm

# --------------------------------
# å®šç¾©å‘½ä»¤åˆ—åƒæ•¸ï¼ˆå¯è‡ªè¡Œä¿®æ”¹é è¨­å€¼ï¼‰
# --------------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="Mamba Virtual Measurement - åƒæ•¸è¨­å®š")
    parser.add_argument("--test_x_path", type=str,
                        default='./testing_data/cnn-2d_2020-09-09_11-45-24_x.npy',
                        help="æ¸¬è©¦è³‡æ–™ X çš„è·¯å¾‘")
    parser.add_argument("--test_y_path", type=str,
                        default='./testing_data/cnn-2d_2020-09-09_11-45-24_y.npy',
                        help="æ¸¬è©¦è³‡æ–™ Y çš„è·¯å¾‘")
    parser.add_argument("--checkpoint_path", type=str,
                        default="checkpoint/123.hdf5",
                        help="æ¨¡å‹ checkpoint çš„è·¯å¾‘")
    parser.add_argument("--mean", type=float, default=65.0,
                        help="mean æ•¸å€¼")
    parser.add_argument("--boundary_upper", type=float, default=70.0,
                        help="ä¸Šé‚Šç•Œæ•¸å€¼")
    parser.add_argument("--boundary_lower", type=float, default=60.0,
                        help="ä¸‹é‚Šç•Œæ•¸å€¼")
    return parser.parse_args()

# --------------------------------
# å®šç¾©æ¨¡å‹åƒæ•¸
# --------------------------------
@dataclass
class ModelArgs:
    model_input_dims: int = 128   # æ¯å€‹ token æŠ•å½±å¾Œçš„ç¶­åº¦
    model_states: int = 32        # èˆ‡åŸæ¶æ§‹ç›¸å®¹ï¼Œæ­¤è™•æœªå¯¦éš›ä½¿ç”¨
    num_layers: int = 5           # å›ºå®šç‚º 5
    dropout_rate: float = 0.2     # (æ­¤ç¯„ä¾‹ä¸­æœªä½¿ç”¨ï¼Œå¯è‡ªè¡Œæ·»åŠ )
    num_classes: int = 1          # è¼¸å‡ºå–®ä½æ•¸ï¼Œè¿´æ­¸ä»»å‹™é€šå¸¸ç‚º 1
    loss: str = 'mse'             # æ­¤è™•ä¸æœƒç”¨å­—ä¸²ï¼Œè€Œæ˜¯æœƒå‚³å…¥è‡ªå®šç¾© loss å‡½æ•¸
    final_activation = None       # è¿´æ­¸ä»»å‹™ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•¸

# --------------------------------
# å®šç¾© Self-Attention æ¨¡çµ„ï¼ˆæ›´æ–°ï¼šå¯å›å‚³æ³¨æ„åŠ›æ¬Šé‡ï¼‰
# --------------------------------
class SelfAttention(layers.Layer):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.q_proj = layers.Dense(dim)
        self.k_proj = layers.Dense(dim)
        self.v_proj = layers.Dense(dim)
        self.softmax = layers.Softmax(axis=-1)
        self.out_proj = layers.Dense(dim)

    # æ–°å¢ return_attention åƒæ•¸ï¼Œè‹¥ True å‰‡å›å‚³ (output, attention_scores)
    def call(self, query, key_value, return_attention=False):
        Q = self.q_proj(query)
        K = self.k_proj(key_value)
        V = self.v_proj(key_value)
        attention_scores = self.softmax(tf.matmul(Q, K, transpose_b=True))
        attention_output = tf.matmul(attention_scores, V)
        output = self.out_proj(attention_output)
        if return_attention:
            return output, attention_scores
        else:
            return output
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "dim": self.dim  # æŠŠå±¤çš„åˆå§‹åŒ–åƒæ•¸ä¿å­˜ä¸‹ä¾†
        })
        return config


# --------------------------------
# å®šç¾© SSMCausalConvBlock æ¨¡çµ„
# --------------------------------
class SSMCausalConvBlock(layers.Layer):
    def __init__(self, modelargs: ModelArgs):
        super().__init__()
        self.args = modelargs
        self.modelargs = modelargs
        # SSM åƒæ•¸ï¼ˆéš¨æ©Ÿåˆå§‹åŒ–ï¼Œåƒ…ä¾›ç¤ºç¯„ï¼‰
        self.A_log = tf.Variable(tf.random.normal([modelargs.model_input_dims, modelargs.model_states]))
        self.D = tf.Variable(tf.ones([modelargs.model_input_dims]))
        self.x_projection = layers.Dense(modelargs.model_states * 2 + modelargs.model_input_dims, use_bias=False)
        self.delta_t_projection = layers.Dense(modelargs.model_input_dims, use_bias=True)
        # å› æœæ²ç©ï¼ˆä½¿ç”¨ causal padding ä¿æŒæ™‚é–“é †åºï¼‰
        self.conv1 = layers.Conv1D(filters=modelargs.model_input_dims, kernel_size=3, padding='causal', activation='relu')
        # SSM è¼¸å‡ºå±¤
        self.linear_filter = layers.Dense(modelargs.model_input_dims, activation='relu')

    def call(self, x, h_t_minus_1):
        A = -tf.exp(self.A_log)
        D = self.D
        x_dbl = self.x_projection(x)
        # åˆ†å‰²ç‚º delta, B, C
        delta, B, C = tf.split(x_dbl, [self.args.model_input_dims, self.args.model_states, self.args.model_states], axis=-1)
        delta = tf.nn.softplus(self.delta_t_projection(delta))
        # è¨ˆç®—æ–°çš„éš±è—ç‹€æ…‹ h_t
        h_t = self.selective_scan(h_t_minus_1, delta, A, B, C, D)
        ssm_output = self.linear_filter(h_t)
        causal_output = self.conv1(ssm_output)
        fused_output = ssm_output * causal_output
        return fused_output, h_t

    def selective_scan(self, h_t_minus_1, delta, A, B, C, D):
        dA = tf.einsum('bld,dn->bldn', delta, A)
        dB_u = tf.einsum('bld,bld,bln->bldn', delta, h_t_minus_1, B)
        dA_cumsum = tf.exp(tf.cumsum(dA, axis=1, exclusive=True))
        x = tf.einsum('bldn,bln->bld', dB_u * dA_cumsum, C)
        return x + x * D
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "modelargs": self.args.__dict__  # å°‡ ModelArgs è½‰ç‚ºå­—å…¸æ ¼å¼
        })
        return config

# --------------------------------
# å®šç¾© MambaBlockï¼ˆä½¿ç”¨ SSMCausalConvBlockï¼‰
# --------------------------------
class MambaBlock(layers.Layer):
    def __init__(self, modelargs: ModelArgs, **kwargs):
        super().__init__(**kwargs)
        self.modelargs = modelargs  # å„²å­˜åƒæ•¸
        self.conv1 = layers.Conv1D(
            filters=modelargs.model_input_dims, 
            kernel_size=3, 
            padding='same', 
            activation='relu'
        )
        self.ssm_block = SSMCausalConvBlock(modelargs)
        self.out_proj = layers.Dense(modelargs.model_input_dims)
    
    def call(self, x):
        x_conv = self.conv1(x)
        h0 = x_conv  # ä½¿ç”¨ conv1 è¼¸å‡ºä½œç‚ºåˆå§‹éš±è—ç‹€æ…‹
        ssm_out, _ = self.ssm_block(x_conv, h0)
        return self.out_proj(ssm_out)

    def get_config(self):
        config = super().get_config()
        config.update({
            "modelargs": self.modelargs.__dict__
        })
        return config
"""class MambaBlock(layers.Layer):
    def __init__(self, modelargs: ModelArgs):
        super().__init__()
        self.conv1 = layers.Conv1D(
            filters=modelargs.model_input_dims, 
            kernel_size=3, 
            padding='same', 
            activation='relu'
        )
        self.ssm_block = SSMCausalConvBlock(modelargs)
        self.out_proj = layers.Dense(modelargs.model_input_dims)

    def call(self, x):
        x = self.conv1(x)
        h0 = tf.zeros_like(x)
        ssm_out, _ = self.ssm_block(x, h0)
        return self.out_proj(ssm_out)"""

# --------------------------------
# å®šç¾©å®Œæ•´æ¨¡å‹çµæ§‹ (Mamba Virtual Measurement)
# ä¸¦åŠ å…¥ return_attention åƒæ•¸ï¼Œä½¿æ¨¡å‹å¯ä»¥å›å‚³æ³¨æ„åŠ›åœ– att1 èˆ‡ att2
# --------------------------------
def build_model(args: ModelArgs, return_attention=False):
    input_layer = layers.Input(shape=(9, 9, 1), name='input')
    seq_length = 9 * 9
    x = layers.Reshape((seq_length, 1))(input_layer)
    x = layers.Dense(args.model_input_dims)(x)
    
    # Middle branch (mean)
    middle_output = MambaBlock(args)(x)
    
    # Upper branchï¼šåˆ©ç”¨å¸¶æœ‰æ³¨æ„åŠ›å›å‚³çš„ SelfAttention å±¤
    upper_sa = SelfAttention(args.model_input_dims)
    if return_attention:
        attention_upper_out, attn_upper_scores = upper_sa(middle_output, x, return_attention=True)
    else:
        attention_upper_out = upper_sa(middle_output, x)
    upper_input = layers.Add()([x, attention_upper_out])
    upper_output = MambaBlock(args)(upper_input)
    
    # Lower branchï¼šåŒç†
    lower_sa = SelfAttention(args.model_input_dims)
    if return_attention:
        attention_lower_out, attn_lower_scores = lower_sa(middle_output, x, return_attention=True)
    else:
        attention_lower_out = lower_sa(middle_output, x)
    lower_input = layers.Add()([x, attention_lower_out])
    lower_output = MambaBlock(args)(lower_input)
    
    middle_flat = layers.Flatten()(middle_output)
    upper_flat = layers.Flatten()(upper_output)
    lower_flat = layers.Flatten()(lower_output)
    
    mean_pred = layers.Dense(args.num_classes, activation=args.final_activation, name='mean')(middle_flat)
    upper_pred = layers.Dense(args.num_classes, activation=args.final_activation, name='upper')(upper_flat)
    lower_pred = layers.Dense(args.num_classes, activation=args.final_activation, name='lower')(lower_flat)
    
    if return_attention:
        # ç”±æ–¼æ³¨æ„åŠ›åˆ†æ•¸çš„ shape ç‚º (batch, 81, 81)ï¼Œ
        # é€™è£¡å–å‡ºæ¯å€‹ sample ä¸­ç¬¬ä¸€å€‹ query token çš„æ³¨æ„åŠ›ï¼ˆshape: (batch, 81)ï¼‰ï¼Œ
        # å† reshape ç‚º (batch, 1, 9, 9) ä¾›å¾ŒçºŒç†±åœ–ä½¿ç”¨
        att_upper_map = layers.Lambda(lambda t: tf.reshape(t[:, 0, :], (-1, 1, 9, 9)), name='att_upper')(attn_upper_scores)
        att_lower_map = layers.Lambda(lambda t: tf.reshape(t[:, 0, :], (-1, 1, 9, 9)), name='att_lower')(attn_lower_scores)
        outputs = [mean_pred, upper_pred, lower_pred, att_upper_map, att_lower_map]
    else:
        outputs = [mean_pred, upper_pred, lower_pred]
    
    model = Model(inputs=input_layer, outputs=outputs, name='Mamba_Virtual_Measurement')
    adam = keras.optimizers.Adam(learning_rate=0.0001)
    
    if return_attention:
        # ç•¶é™„åŠ è¼¸å‡ºæ³¨æ„åŠ›åœ–æ™‚ï¼Œè¨­å®š dummy loss ä½¿å…¶ä¸å½±éŸ¿è¨“ç·´
        def dummy_loss(y_true, y_pred):
            return tf.zeros_like(y_pred)
        model.compile(loss=[loss_mse, dummy_loss, dummy_loss, dummy_loss, dummy_loss], optimizer=adam)
    else:
        model.compile(loss=loss_mse, optimizer=adam)
    
    return model

# --------------------------------
# è‡ªå®šç¾© Loss å‡½æ•¸ï¼ˆå…¨éƒ¨æ”¹ç‚ºç´” TensorFlow é‹ç®—ï¼Œä¸ä½¿ç”¨ .numpy()ï¼‰
# --------------------------------
def loss_mse(y_true, y_pred):
    theta = 1.0
    total_loss = 0.0
    # åˆ†æ”¯ 0ï¼šmean ç›´æ¥ç”¨ MSE
    l0 = MSE(y_pred[0], y_true) if isinstance(y_pred, list) else MSE(y_pred, y_true)
    total_loss += l0
    # åˆ†æ”¯ 1ï¼šupper
    raw = y_true - 65.0
    lambda_val = tf.where(raw < theta, 0.0, raw)
    lambda_base = tf.where(lambda_val > theta, 1.0, lambda_val)
    lambda_modified = tf.where(tf.equal(lambda_val, 0.0),
                               0.0,
                               tf.where(y_true >= 70.0, 1.0, tf.math.tanh(lambda_val)))
    total_lambda = lambda_base + lambda_modified
    l1 = loss_mse_lambda(y_pred[1] if isinstance(y_pred, list) else y_pred, y_true, total_lambda)
    total_loss += l1
    # åˆ†æ”¯ 2ï¼šlower
    raw = 65.0 - y_true
    lambda_val = tf.where(raw < theta, 0.0, raw)
    lambda_base = tf.where(lambda_val > theta, 1.0, lambda_val)
    lambda_modified = tf.where(tf.equal(lambda_val, 0.0),
                               0.0,
                               tf.where(y_true <= 60.0, 1.0, tf.math.tanh(lambda_val)))
    total_lambda = lambda_base + lambda_modified
    l2 = loss_mse_lambda(y_pred[2] if isinstance(y_pred, list) else y_pred, y_true, total_lambda)
    total_loss += l2
    return total_loss

def MSE(y_pred, y_true):  
    return tf.reduce_mean(tf.square(y_true - y_pred))

def loss_mse_lambda(outputs, targets, _lambda):
    # outputs, targets, _lambda å‡å½¢ç‹€ç‚º [batch, 1]
    sq_error = tf.square(outputs - targets)
    weighted_loss = _lambda * sq_error
    total_loss = tf.reduce_sum(weighted_loss)
    count = tf.reduce_sum(tf.cast(_lambda > 0, tf.float32))
    count = tf.maximum(count, 1.0)
    return total_loss / count

# --------------------------------
# è¼‰å…¥è¨“ç·´èˆ‡é©—è­‰è³‡æ–™ï¼ˆé€™éƒ¨åˆ†è·¯å¾‘å›ºå®šï¼Œå¯ä¾éœ€æ±‚è‡ªè¡Œæ”¹ï¼‰
# --------------------------------
CP_data_train_2d_X = np.load('./training_data/PETBottle/cnn-2d_2020-09-09_11-45-24_x.npy')
CP_data_train_2d_Y = np.load('./training_data/PETBottle/cnn-2d_2020-09-09_11-45-24_y.npy')
CP_data_train_2d_X_tensor = tf.convert_to_tensor(CP_data_train_2d_X)
CP_data_train_2d_Y_tensor = tf.convert_to_tensor(CP_data_train_2d_Y)
train_X_2d = tf.expand_dims(CP_data_train_2d_X_tensor, axis=3)
train_Y_2d = CP_data_train_2d_Y_tensor

CP_data_valid_2d_X = np.load('./validation_data/PETBottle/cnn-2d_2020-09-09_11-45-24_x.npy')
CP_data_valid_2d_Y = np.load('./validation_data/PETBottle/cnn-2d_2020-09-09_11-45-24_y.npy')
CP_data_valid_2d_X_tensor = tf.convert_to_tensor(CP_data_valid_2d_X)
CP_data_valid_2d_Y_tensor = tf.convert_to_tensor(CP_data_valid_2d_Y)
valid_X_2d = tf.expand_dims(CP_data_valid_2d_X_tensor, axis=3)
valid_Y_2d = CP_data_valid_2d_Y_tensor

# --------------------------------
# å®šç¾©ç†±åœ–ç¹ªè£½å‡½æ•¸
# --------------------------------
def heatmap_drawer(attention, gt_value, pred_value, pred_cls, maps, result_path):
    """
    ç¹ªè£½æ³¨æ„åŠ›ç†±åœ–ä¸¦å„²å­˜è‡³ result_path ä¸­çš„ AN_U èˆ‡ AN_L å­è³‡æ–™å¤¾
    attention: listï¼ŒåŒ…å«å…©å€‹ numpy é™£åˆ—ï¼Œåˆ†åˆ¥ç‚ºä¸Šé‚Šç•Œèˆ‡ä¸‹é‚Šç•Œæ³¨æ„åŠ›åœ–ï¼Œå½¢ç‹€é æœŸ [num_samples, 1, H, W]
    gt_value: çœŸå¯¦å€¼ numpy é™£åˆ—ï¼Œå½¢ç‹€ [num_samples,]
    pred_value: é æ¸¬å€¼ numpy é™£åˆ—ï¼Œå½¢ç‹€ [num_samples,]
    pred_cls: é æ¸¬åˆ†é¡ numpy é™£åˆ—ï¼Œå½¢ç‹€ [num_samples,]ï¼Œä¸Šé‚Šç•Œæ‡‰ç‚º 1ï¼Œä¸‹é‚Šç•Œæ‡‰ç‚º 2
    maps: y è»¸æ¨™ç±¤ list
    result_path: çµæœå„²å­˜çš„æ ¹ç›®éŒ„
    """
    # squeeze æ‰æ³¨æ„åŠ›åœ–ç¬¬äºŒç¶­åº¦
    an_u = np.squeeze(attention[0], axis=1)  # ä¸Šé‚Šç•Œæ³¨æ„åŠ›åœ–, shape: (num_samples, H, W)
    an_l = np.squeeze(attention[1], axis=1)  # ä¸‹é‚Šç•Œæ³¨æ„åŠ›åœ–, shape: (num_samples, H, W)
    
    # å»ºç«‹çµæœè³‡æ–™å¤¾
    upper_path = os.path.join(result_path, "AN_U")
    lower_path = os.path.join(result_path, "AN_L")
    os.makedirs(upper_path, exist_ok=True)
    os.makedirs(lower_path, exist_ok=True)
    
    # ä¸Šé‚Šç•Œç†±åœ–ç¹ªè£½
    for idx, att_map in enumerate(tqdm(an_u, desc="Upper branch heatmaps")):
        _gt_value = np.round(gt_value[idx], 2)
        _pred_value = np.round(pred_value[idx], 2)
        if pred_cls[idx] != 1:  # åƒ…é‡å°åˆ†é¡ç‚º 1 çš„ä¸Šé‚Šç•Œæ¨£æœ¬
            continue
        att_map_copy = att_map.copy()
        if att_map_copy.shape[0] > 0:
            att_map_copy[0, 1:] = -1
        plt.figure(figsize=(12, 8))
        ax = sns.heatmap(att_map_copy, xticklabels=list("123456789"), yticklabels=maps,
                         linewidths=0.1, cbar=True, square=False)
        title = f"{idx}_gt-{_gt_value}_pred-{_pred_value}"
        plt.title(title, fontsize="x-large")
        plt.ylabel("Process Data", fontsize="x-large")
        # èª¿æ•´å·¦å´ç•™ç™½ä»¥é¡¯ç¤ºå®Œæ•´æ¨™ç±¤
        plt.subplots_adjust(left=0.3)
        plt.savefig(os.path.join(upper_path, f"{title}.svg"))
        plt.close()
    
    # ä¸‹é‚Šç•Œç†±åœ–ç¹ªè£½
    for idx, att_map in enumerate(tqdm(an_l, desc="Lower branch heatmaps")):
        _gt_value = np.round(gt_value[idx], 2)
        _pred_value = np.round(pred_value[idx], 2)
        if pred_cls[idx] != 2:  # åƒ…é‡å°åˆ†é¡ç‚º 2 çš„ä¸‹é‚Šç•Œæ¨£æœ¬
            continue
        att_map_copy = att_map.copy()
        if att_map_copy.shape[0] > 0:
            att_map_copy[0, 1:] = -1
        plt.figure(figsize=(12, 8))
        ax = sns.heatmap(att_map_copy, xticklabels=list("123456789"), yticklabels=maps,
                         linewidths=0.1, cbar=True, square=False)
        title = f"{idx}_gt-{_gt_value}_pred-{_pred_value}"
        plt.title(title, fontsize="x-large")
        plt.ylabel("Process Data", fontsize="x-large")
        plt.subplots_adjust(left=0.3)
        plt.savefig(os.path.join(lower_path, f"{title}.svg"))
        plt.close()

# --------------------------------
# ä¸»ç¨‹å¼å€å¡Šï¼šè¼‰å…¥æ¸¬è©¦è³‡æ–™ã€checkpoint èˆ‡å…¶ä»–åƒæ•¸ï¼Œé€²è¡Œæ¨¡å‹é æ¸¬èˆ‡çµæœè©•ä¼°ï¼ŒåŒæ™‚ç¹ªè£½ç†±åœ–
# --------------------------------
if __name__ == '__main__':
    args_cli = parse_args()
    
    CP_data_test_2d_X = np.load(args_cli.test_x_path)
    CP_data_test_2d_Y = np.load(args_cli.test_y_path)
    CP_data_test_2d_X_tensor = tf.convert_to_tensor(CP_data_test_2d_X)
    CP_data_test_2d_Y_tensor = tf.convert_to_tensor(CP_data_test_2d_Y)
    test_X_2d = tf.expand_dims(CP_data_test_2d_X_tensor, axis=3)
    test_Y_2d = CP_data_test_2d_Y_tensor 

    checkpoint_path = args_cli.checkpoint_path
    mean = args_cli.mean
    boundary_upper = args_cli.boundary_upper
    boundary_lower = args_cli.boundary_lower

    theta = 2.0
    mean_range_upper = mean + theta
    mean_range_lower = mean - theta

    model_args = ModelArgs(
        model_input_dims=128,
        model_states=32,
        num_layers=5,
        dropout_rate=0.2,
        num_classes=1,
        loss=loss_mse
    )
    # è¨­å®š return_attention=True ä»¥ä¾¿æ–¼å–å¾—æ³¨æ„åŠ›åœ–ä¾›ç†±åœ–ç¹ªè£½
    model1 = build_model(model_args, return_attention=True)
    # å˜—è©¦è¼‰å…¥æ¬Šé‡ï¼Œä½¿ç”¨ skip_mismatch é¿å…éŒ¯èª¤
    print("ğŸ” å˜—è©¦è¼‰å…¥å·²è¨“ç·´æ¬Šé‡...")
    #model1.load_weights(checkpoint_path)  # å…ˆå˜—è©¦ä¸è·³é mismatch
    model1.load_weights(checkpoint_path, by_name=True, skip_mismatch=True)
    
    # æª¢æŸ¥ MambaBlock å±¤çš„æ¬Šé‡æ˜¯å¦èˆ‡ .h5 æ¬Šé‡æª”åŒ¹é…
    print("\nğŸ” æª¢æŸ¥ MambaBlock æ¬Šé‡è¼‰å…¥æƒ…æ³ï¼š")
    for layer in model1.layers:
        if "mamba_block" in layer.name:
            print(f"Layer: {layer.name}")
            for weight in layer.weights:
                print(f"- {weight.name}: {weight.shape}, mean: {tf.reduce_mean(weight).numpy():.6f}, std: {tf.math.reduce_std(weight).numpy():.6f}")
    
    # é¡¯ç¤ºå®Œæ•´æ¨¡å‹æ¶æ§‹
    print("\nğŸ” æ¨¡å‹æ¶æ§‹ç¸½è¦½ï¼š")
    model1.summary()

    # ä½¿ç”¨æ¨¡å‹é æ¸¬ï¼Œpred_y ä¾åºç‚º [mean_pred, upper_pred, lower_pred, att_upper_map, att_lower_map]
    pred_y = model1.predict(test_X_2d)
    mean_pred = pred_y[0]
    print("mean_pred shape:", mean_pred.shape)

    # ---------------------- åˆ©ç”¨ mean åˆ†æ”¯é æ¸¬ä½œç‚ºåˆ†é¡å™¨ ----------------------
    pred_test_net_cls = np.copy(mean_pred)
    for idx in range(pred_test_net_cls.shape[0]):
        data = pred_test_net_cls[idx, 0]
        if mean_range_lower <= data <= mean_range_upper:
            pred_test_net_cls[idx, 0] = 0
        elif data > mean_range_upper:
            pred_test_net_cls[idx, 0] = 1
        elif data < mean_range_lower:
            pred_test_net_cls[idx, 0] = 2
    pred_test_net_cls = np.squeeze(pred_test_net_cls).astype('int64')

    # åˆ©ç”¨åˆ†é¡çµæœæ•´åˆ RACNN èˆ‡ Classifier é æ¸¬
    pred_test_net_combine = np.copy(mean_pred)
    for idx, cls_result in enumerate(pred_test_net_cls):
        # è‹¥åˆ†é¡çµæœç‚º 0 ä½¿ç”¨ mean_predï¼Œ1 ä½¿ç”¨ upper_predï¼Œ2 ä½¿ç”¨ lower_pred
        pred_test_net_combine[idx, 0] = pred_y[cls_result][idx, 0]
    pred_test_net_combine = tf.convert_to_tensor(pred_test_net_combine)

    # ---------------------- BN0 æ··æ·†çŸ©é™£ (ä½¿ç”¨ mean åˆ†æ”¯é æ¸¬) ----------------------
    confusion_matrix = np.zeros((3, 3))
    for idx in range(mean_pred.shape[0]):
        gt_y_value = test_Y_2d[idx].numpy()
        pred_y_value = mean_pred[idx, 0]
        if mean_range_lower <= gt_y_value <= mean_range_upper:
            if mean_range_lower <= pred_y_value <= mean_range_upper:
                confusion_matrix[0, 0] += 1
            elif (mean_range_upper < pred_y_value < boundary_upper) or (mean_range_lower > pred_y_value > boundary_lower):
                confusion_matrix[0, 1] += 1
            elif pred_y_value >= boundary_upper or pred_y_value <= boundary_lower:
                confusion_matrix[0, 2] += 1
            else:
                print("Dataä¼¼ä¹æœ‰å•é¡Œ0ã€‚")
        elif (mean_range_upper < gt_y_value < boundary_upper) or (mean_range_lower > gt_y_value > boundary_lower):
            if mean_range_lower <= pred_y_value <= mean_range_upper:
                confusion_matrix[1, 0] += 1
            elif (mean_range_upper < pred_y_value < boundary_upper) or (mean_range_lower > pred_y_value > boundary_lower):
                confusion_matrix[1, 1] += 1
            elif pred_y_value >= boundary_upper or pred_y_value <= boundary_lower:
                confusion_matrix[1, 2] += 1
            else:
                print("Dataä¼¼ä¹æœ‰å•é¡Œ1ã€‚")
        elif gt_y_value >= boundary_upper or gt_y_value <= boundary_lower:
            if mean_range_lower <= pred_y_value <= mean_range_upper:
                confusion_matrix[2, 0] += 1
            elif (mean_range_upper < pred_y_value < boundary_upper) or (mean_range_lower > pred_y_value > boundary_lower):
                confusion_matrix[2, 1] += 1
            elif pred_y_value >= boundary_upper or pred_y_value <= boundary_lower:
                confusion_matrix[2, 2] += 1
            else:
                print("Dataä¼¼ä¹æœ‰å•é¡Œ2ã€‚")
        else:
            print("Dataä¼¼ä¹æœ‰å•é¡Œ3ã€‚")
    print("BN0 confusion matrix (mean branch):")
    print(confusion_matrix)

    confusion_matrix_col = np.sum(confusion_matrix, axis=0)
    confusion_matrix_row = np.sum(confusion_matrix, axis=1)

    false_alarm_probability = (confusion_matrix[0,1] + confusion_matrix[0,2] +
                                 confusion_matrix[1,0] + confusion_matrix[1,2] +
                                 confusion_matrix[2,0] + confusion_matrix[2,1]) / np.sum(confusion_matrix_col)
    print(f"False Alarm Probability: {false_alarm_probability}")
    precision = np.array([
        confusion_matrix[0, 0] / confusion_matrix_col[0] if confusion_matrix_col[0] != 0 else 0,
        confusion_matrix[1, 1] / confusion_matrix_col[1] if confusion_matrix_col[1] != 0 else 0,
        confusion_matrix[2, 2] / confusion_matrix_col[2] if confusion_matrix_col[2] != 0 else 0
    ])
    print(f"precision: {precision}")
    recall = np.array([confusion_matrix[0,0]/confusion_matrix_row[0],
                       confusion_matrix[1,1]/confusion_matrix_row[1],
                       confusion_matrix[2,2]/confusion_matrix_row[2]])
    print(f"recall: {recall}")
    accuracy = (confusion_matrix[0,0] + confusion_matrix[1,1] + confusion_matrix[2,2]) / np.sum(confusion_matrix_row)
    print(f"accuracy: {accuracy}")

    # ---------------------- é‚Šç•Œæª¢æ¸¬ä¹‹é™£åˆ—èˆ‡è©•ä¼°å‡½æ•¸ ----------------------
    detect_range = 4
    detect_precision = 0.5
    interval_upper = []
    interval_lower = []
    split_interval = int(detect_range / detect_precision)
    for idx in range(1, split_interval + 1):
        interval_upper.append([boundary_upper - 0.5 * idx, boundary_upper + 0.5 * idx, 0.5 * idx])
        interval_lower.append([boundary_lower - 0.5 * idx, boundary_lower + 0.5 * idx, 0.5 * idx])
    interval_upper.append([boundary_upper, boundary_upper + 999.0, 0.0])
    interval_lower.append([boundary_lower - 999.0, boundary_lower, 0.0])

    def MAE(f_x, y):
        num = len(f_x)
        summation = sum(abs(f_x - y))
        result = summation / num
        return result

    def RMSE(f_x, y):
        num = len(f_x)
        summation = sum((f_x - y) ** 2)
        result = (summation / num) ** 0.5
        return result

    def MAPE(y_true, y_pred):
        n = len(y_true)
        mape_val = sum(np.abs((y_true - y_pred) / y_true)) / n * 100
        return mape_val

    # ------------- BN0 é‚Šç•Œæª¢æ¸¬ -------------
    interval_upper_detect_result = []
    for idx1, data in enumerate(interval_upper):
        interval_upper_test_data = []
        interval_upper_pred_data = []
        for idx2, y in enumerate(pred_test_net_combine.numpy()):
            if data[0] <= y[0] <= data[1]:
                interval_upper_test_data.append(test_Y_2d[idx2].numpy())
                interval_upper_pred_data.append(mean_pred[idx2, 0])
        if len(interval_upper_test_data) > 0 and len(interval_upper_pred_data) > 0:
            mean_absolute_error = MAE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data))
            root_mean_square_error = RMSE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data))
            mape_val = MAPE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data))
            max_error = np.max(np.abs(np.array(interval_upper_test_data) - np.array(interval_upper_pred_data)))
        else:
            mean_absolute_error, root_mean_square_error, mape_val, max_error = 0.0, 0.0, 0.0, 0.0
        interval_upper_detect_result.append(
                [np.array(data), 
                 np.array(interval_upper_test_data), 
                 np.array(interval_upper_pred_data),
                 np.array([mean_absolute_error, root_mean_square_error, mape_val, max_error])]
            )
    """
    #æª¢æŸ¥ interval_upper_detect_result çš„å…§å®¹
    for i, item in enumerate(interval_upper_detect_result):
        print(f"Index {i}: {type(item)}, Length: {len(item) if isinstance(item, list) else 'N/A'}")
    #--------------------------------------
    #ç¢ºèªå…§éƒ¨æ•¸æ“šçµæ§‹
    for i, item in enumerate(interval_upper_detect_result):
        print(f"Index {i}: Type: {type(item)}, Length: {len(item)}")
        for j, sub_item in enumerate(item):
            print(f"  â”œâ”€â”€ Element {j}: Type: {type(sub_item)}, Shape: {sub_item.shape if isinstance(sub_item, np.ndarray) else 'N/A'}")
    #-------------
    """

    max_len = max(max(len(sub_item) for sub_item in item) for item in interval_upper_detect_result)

    # è®“æ‰€æœ‰å…ƒç´ å¡«å……åˆ°ç›¸åŒé•·åº¦
    interval_upper_detect_result = [
        [np.pad(sub_item, (0, max_len - len(sub_item)), constant_values=0) if len(sub_item) < max_len else sub_item
        for sub_item in item]
        for item in interval_upper_detect_result
    ]

    # è½‰æˆ NumPy é™£åˆ—
    interval_upper_detect_result = np.array(interval_upper_detect_result)

    interval_lower_detect_result = []
    for idx1, data in enumerate(interval_lower):
        interval_lower_test_data = []
        interval_lower_pred_data = []
        for idx2, y in enumerate(pred_test_net_combine.numpy()):
            if data[0] <= y[0] <= data[1]:
                interval_lower_test_data.append(test_Y_2d[idx2].numpy())
                interval_lower_pred_data.append(mean_pred[idx2, 0])
        if len(interval_lower_test_data) > 0 and len(interval_lower_pred_data) > 0:
            mean_absolute_error = MAE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data))
            root_mean_square_error = RMSE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data))
            mape_val = MAPE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data))
            max_error = np.max(np.abs(np.array(interval_lower_test_data) - np.array(interval_lower_pred_data)))
        else:
            mean_absolute_error, root_mean_square_error, mape_val, max_error = 0.0, 0.0, 0.0, 0.0

        # ç¢ºä¿ interval_lower_test_data å’Œ interval_lower_pred_data å½¢ç‹€ä¸€è‡´
        interval_lower_test_data = np.array(interval_lower_test_data)  # ç¢ºä¿è½‰æ›ç‚º NumPy é™£åˆ—
        interval_lower_pred_data = np.array(interval_lower_pred_data)  # ç¢ºä¿è½‰æ›ç‚º NumPy é™£åˆ—

        interval_lower_test_data = np.squeeze(interval_lower_test_data) if interval_lower_test_data.size > 0 else np.array([0.0])
        interval_lower_pred_data = np.squeeze(interval_lower_pred_data) if interval_lower_pred_data.size > 0 else np.array([0.0])

        # ç¢ºä¿æ•¸æ“šç‚º float
        """
        mean_absolute_error = float(mean_absolute_error)
        root_mean_square_error = float(root_mean_square_error)
        mape_val = float(mape_val)
        max_error = float(max_error)
        """
        # --------------
        # ç¢ºä¿ interval_lower_test_data å’Œ interval_lower_pred_data å½¢ç‹€ä¸€è‡´
        """
        print(f"interval_lower_test_data shape: {np.array(interval_lower_test_data).shape}")
        print(f"interval_lower_pred_data shape: {np.array(interval_lower_pred_data).shape}")
        """
        # ----------------------------------------------------------------
        # ç™¼ç¾å½¢ç‹€ä¸ä¸€è‡´ï¼Œå‰‡å¯ä»¥çµ±ä¸€è½‰æ›ç‚ºä¸€ç¶­é™£åˆ—ï¼š
        interval_lower_test_data = np.array(interval_lower_test_data).flatten()
        interval_lower_pred_data = np.array(interval_lower_pred_data).flatten()
        # ----------------------------------
        # ç¢ºä¿ mean_absolute_error ç­‰æ•¸æ“šç‚º float
        mean_absolute_error = np.array(mean_absolute_error).flatten()
        root_mean_square_error = np.array(root_mean_square_error).flatten()
        mape_val = np.array(mape_val).flatten()
        max_error = np.array(max_error).flatten()

        mean_absolute_error = float(mean_absolute_error[0]) if mean_absolute_error.size > 0 else 0.0
        root_mean_square_error = float(root_mean_square_error[0]) if root_mean_square_error.size > 0 else 0.0
        mape_val = float(mape_val[0]) if mape_val.size > 0 else 0.0
        max_error = float(max_error[0]) if max_error.size > 0 else 0.0
        # ----------------------------------
        interval_lower_detect_result.append(
                [np.array(data), 
                np.array(interval_lower_test_data), 
                np.array(interval_lower_pred_data),
                np.array([mean_absolute_error, root_mean_square_error, mape_val, max_error])]
            )
    """# ç¢ºä¿æ‰€æœ‰å…ƒç´ å½¢ç‹€ä¸€è‡´
    max_len = max(len(item) for item in interval_lower_detect_result)

    # è®“æ‰€æœ‰å…ƒç´ å¡«å……åˆ°ç›¸åŒé•·åº¦
    interval_lower_detect_result = [
        np.pad(item, (0, max_len - len(item)), constant_values=0) if isinstance(item, np.ndarray) and len(item) < max_len else item
        for item in interval_lower_detect_result
    ]
    # --------------------"""
    """
    # æª¢æŸ¥ä¸¦ç¢ºä¿æ•¸æ“šå½¢ç‹€ä¸€è‡´
    for i, item in enumerate(interval_lower_detect_result):
        print(f"Index {i}: Type: {type(item)}, Length: {len(item) if hasattr(item, '__len__') else 'N/A'}")
        for j, sub_item in enumerate(item):
            print(f"  â”œâ”€â”€ Element {j}: Type: {type(sub_item)}, Shape: {sub_item.shape if isinstance(sub_item, np.ndarray) else 'N/A'}")
    """
    # è½‰æ›æˆ NumPy ç‰©ä»¶é™£åˆ—ï¼Œå…è¨±ä¸åŒå½¢ç‹€
    interval_lower_detect_result = np.array(interval_lower_detect_result, dtype=object)
    #interval_lower_detect_result = np.array(interval_lower_detect_result)

    with open('result_data/(BN0)bonduary(upper+lower)_detect_result.csv', 'w') as f:
        f.write(f"########,ä¸Šé‚Šç•Œ({boundary_upper}),å€é–“æª¢æ¸¬,çµæœ,########\n")
        for idx, (low, up, bound_val) in enumerate(interval_upper):
            f.write(f",({idx+1}) {boundary_upper}Â±{bound_val} [{low}~{up}]")
        f.write('\nMAE,')
        for (_MAE, _, _, _) in interval_upper_detect_result[:, -1]:
            f.write(f"{_MAE},")
        f.write('\nRMSE,')
        for (_, _RMSE, _, _) in interval_upper_detect_result[:, -1]:
            f.write(f"{_RMSE},")
        f.write('\nMAPE,')
        for (_, _, _MAPE, _) in interval_upper_detect_result[:, -1]:
            f.write(f"{_MAPE},")
        f.write('\nME,')
        for (_, _, _, _ME) in interval_upper_detect_result[:, -1]:
            f.write(f"{_ME},")
        for interval, __, _, (_MAE, _RMSE, _MAPE, _ME) in interval_upper_detect_result:
            print(interval, _MAE, _RMSE, _MAPE, _ME)
        
        f.write(f"\n\n########,ä¸‹é‚Šç•Œ({boundary_lower}),å€é–“æª¢æ¸¬,çµæœ,########\n")
        for idx, (low, up, bound_val) in enumerate(interval_lower):
            f.write(f",({idx+1}) {boundary_lower}Â±{bound_val} [{low}~{up}]")
        f.write('\nMAE,')
        for (_MAE, _, _, _) in interval_lower_detect_result[:, -1]:
            f.write(f"{_MAE},")
        f.write('\nRMSE,')
        for (_, _RMSE, _, _) in interval_lower_detect_result[:, -1]:
            f.write(f"{_RMSE},")
        f.write('\nMAPE,')
        for (_, _, _MAPE, _) in interval_lower_detect_result[:, -1]:
            f.write(f"{_MAPE},")
        f.write('\nME,')
        for (_, _, _, _ME) in interval_lower_detect_result[:, -1]:
            f.write(f"{_ME},")
        
        for interval, __, _, (_MAE, _RMSE, _MAPE, _ME) in interval_lower_detect_result:
            print(interval, _MAE, _RMSE, _MAPE, _ME)
    
    with open('result_data/(BN0)bonduary_detect_result.csv', 'w') as f:
        f.write(f"########,é‚Šç•Œ({boundary_upper}ã€{boundary_lower}),å€é–“æª¢æ¸¬,çµæœ,########\n")
        for idx, (low, up, bound_val) in enumerate(interval_upper):
            f.write(f",({idx+1}) boundaryÂ±{bound_val}")
        f.write('\nMAE,')
        for idx, ((u_MAE, _, _, _), (l_MAE, _, _, _)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            if u_MAE == 0 or l_MAE == 0:
                f.write(f"{(u_MAE+l_MAE)},")
            else:
                f.write(f"{(u_MAE+l_MAE)/2.0},")
        f.write('\nRMSE,')
        for idx, ((_, u_RMSE, _, _), (_, l_RMSE, _, _)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            if u_RMSE == 0 or l_RMSE == 0:
                f.write(f"{(u_RMSE+l_RMSE)},")
            else:
                f.write(f"{(u_RMSE+l_RMSE)/2.0},")
        f.write('\nMAPE,')
        for idx, ((_, _, u_MAPE, _), (_, _, l_MAPE, _)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            if u_MAPE == 0 or l_MAPE == 0:
                f.write(f"{(u_MAPE+l_MAPE)},")
            else:
                f.write(f"{(u_MAPE+l_MAPE)/2.0},")
        f.write('\nME,')
        for idx, ((_, _, _, u_ME), (_, _, _, l_ME)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            f.write(f"{np.max((u_ME, l_ME))},")
    
    for idx, data in enumerate(interval_upper_detect_result):
        if (interval_upper_detect_result[idx][-1][0] == 0 or interval_lower_detect_result[idx][-1][0] == 0):
            _MAE = interval_upper_detect_result[idx][-1][0] + interval_lower_detect_result[idx][-1][0]
        else:
            _MAE = (interval_upper_detect_result[idx][-1][0] + interval_lower_detect_result[idx][-1][0]) / 2.0

        if (interval_upper_detect_result[idx][-1][1] == 0 or interval_lower_detect_result[idx][-1][1] == 0):
            _RMSE = interval_upper_detect_result[idx][-1][1] + interval_lower_detect_result[idx][-1][1]
        else:
            _RMSE = (interval_upper_detect_result[idx][-1][1] + interval_lower_detect_result[idx][-1][1]) / 2.0

        if (interval_upper_detect_result[idx][-1][2] == 0 or interval_lower_detect_result[idx][-1][2] == 0):
            _MAPE = interval_upper_detect_result[idx][-1][2] + interval_lower_detect_result[idx][-1][2]
        else:
            _MAPE = (interval_upper_detect_result[idx][-1][2] + interval_lower_detect_result[idx][-1][2]) / 2.0

        _ME = np.max((interval_upper_detect_result[idx][-1][3], interval_lower_detect_result[idx][-1][3]))

        print(f"mean result :: MAE: {_MAE}, RMSE: {_RMSE}, MAPE: {_MAPE}, ME: {_ME}")

    # ------------- RACNN+Classifier é‚Šç•Œæª¢æ¸¬ -------------
    interval_upper_detect_result = []
    for idx1, data in enumerate(interval_upper):
        interval_upper_test_data = []
        interval_upper_pred_data = []
        for idx2, y in enumerate(test_Y_2d):
            if data[0] <= y.numpy() <= data[1]:
                interval_upper_test_data.append(y.numpy())
                interval_upper_pred_data.append(pred_test_net_combine[idx2].numpy())
        if len(interval_upper_test_data) > 0 and len(interval_upper_pred_data) > 0:
            """
            mean_absolute_error = MAE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data))
            root_mean_square_error = RMSE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data))
            mape_val = MAPE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data))
            max_error = np.max(np.abs(np.array(interval_upper_test_data) - np.array(interval_upper_pred_data)))
            """
            mean_absolute_error = MAE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data)).item()
            root_mean_square_error = RMSE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data)).item()
            mape_val = MAPE(np.array(interval_upper_test_data), np.array(interval_upper_pred_data)).item()
            max_error = np.max(np.abs(np.array(interval_upper_test_data) - np.array(interval_upper_pred_data))).item()
        else:
            mean_absolute_error, root_mean_square_error, mape_val, max_error = 0.0, 0.0, 0.0, 0.0
        interval_upper_detect_result.append(
                [np.array(data), 
                np.array(interval_upper_test_data), 
                np.array(interval_upper_pred_data),
                np.array([mean_absolute_error, root_mean_square_error, mape_val, max_error])]
            )
    interval_upper_detect_result = np.array(interval_upper_detect_result, dtype=object)
    # interval_upper_detect_result = np.array(interval_upper_detect_result)

    interval_lower_detect_result = []
    for idx1, data in enumerate(interval_lower):
        interval_lower_test_data = []
        interval_lower_pred_data = []
        for idx2, y in enumerate(test_Y_2d):
            if data[0] <= y.numpy() <= data[1]:
                interval_lower_test_data.append(y.numpy())
                interval_lower_pred_data.append(pred_test_net_combine[idx2].numpy())
        if len(interval_lower_test_data) > 0 and len(interval_lower_pred_data) > 0:
            """
            mean_absolute_error = MAE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data))
            root_mean_square_error = RMSE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data))
            mape_val = MAPE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data))
            max_error = np.max(np.abs(np.array(interval_lower_test_data) - np.array(interval_lower_pred_data)))
            """
            mean_absolute_error = MAE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data)).item()
            root_mean_square_error = RMSE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data)).item()
            mape_val = MAPE(np.array(interval_lower_test_data), np.array(interval_lower_pred_data)).item()
            max_error = np.max(np.abs(np.array(interval_lower_test_data) - np.array(interval_lower_pred_data))).item()
        else:
            mean_absolute_error, root_mean_square_error, mape_val, max_error = 0.0, 0.0, 0.0, 0.0
        interval_lower_detect_result.append(
                [np.array(data), 
                np.array(interval_lower_test_data), 
                np.array(interval_lower_pred_data),
                np.array([mean_absolute_error, root_mean_square_error, mape_val, max_error])]
            )
    interval_lower_detect_result = np.array(interval_lower_detect_result, dtype=object)

    with open('result_data/(RACNN+Classifier)bonduary(upper+lower)_detect_result.csv', 'w') as f:
        f.write(f"########,ä¸Šé‚Šç•Œ({boundary_upper}),å€é–“æª¢æ¸¬,çµæœ,########\n")
        for idx, (low, up, bound_val) in enumerate(interval_upper):
            f.write(f",({idx+1}) {boundary_upper}Â±{bound_val} [{low}~{up}]")
        f.write('\nMAE,')
        for (_MAE, _, _, _) in interval_upper_detect_result[:, -1]:
            f.write(f"{_MAE},")
        f.write('\nRMSE,')
        for (_, _RMSE, _, _) in interval_upper_detect_result[:, -1]:
            f.write(f"{_RMSE},")
        f.write('\nMAPE,')
        for (_, _, _MAPE, _) in interval_upper_detect_result[:, -1]:
            f.write(f"{_MAPE},")
        f.write('\nME,')
        for (_, _, _, _ME) in interval_upper_detect_result[:, -1]:
            f.write(f"{_ME},")
        for interval, __, _, (_MAE, _RMSE, _MAPE, _ME) in interval_upper_detect_result:
            print(interval, _MAE, _RMSE, _MAPE, _ME)
        
        f.write(f"\n\n########,ä¸‹é‚Šç•Œ({boundary_lower}),å€é–“æª¢æ¸¬,çµæœ,########\n")
        for idx, (low, up, bound_val) in enumerate(interval_lower):
            f.write(f",({idx+1}) {boundary_lower}Â±{bound_val} [{low}~{up}]")
        f.write('\nMAE,')
        for (_MAE, _, _, _) in interval_lower_detect_result[:, -1]:
            f.write(f"{_MAE},")
        f.write('\nRMSE,')
        for (_, _RMSE, _, _) in interval_lower_detect_result[:, -1]:
            f.write(f"{_RMSE},")
        f.write('\nMAPE,')
        for (_, _, _MAPE, _) in interval_lower_detect_result[:, -1]:
            f.write(f"{_MAPE},")
        f.write('\nME,')
        for (_, _, _, _ME) in interval_lower_detect_result[:, -1]:
            f.write(f"{_ME},")
    
    with open('result_data/(RACNN+Classifier)bonduary_detect_result.csv', 'w') as f:
        f.write(f"########,é‚Šç•Œ({boundary_upper}ã€{boundary_lower}),å€é–“æª¢æ¸¬,çµæœ,########\n")
        for idx, (low, up, bound_val) in enumerate(interval_upper):
            f.write(f",({idx+1}) boundaryÂ±{bound_val}")
        f.write('\nMAE,')
        for idx, ((u_MAE, _, _, _), (l_MAE, _, _, _)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            if u_MAE == 0 or l_MAE == 0:
                f.write(f"{(u_MAE+l_MAE)},")
            else:
                f.write(f"{(u_MAE+l_MAE)/2.0},")
        f.write('\nRMSE,')
        for idx, ((_, u_RMSE, _, _), (_, l_RMSE, _, _)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            if u_RMSE == 0 or l_RMSE == 0:
                f.write(f"{(u_RMSE+l_RMSE)},")
            else:
                f.write(f"{(u_RMSE+l_RMSE)/2.0},")
        f.write('\nMAPE,')
        for idx, ((_, _, u_MAPE, _), (_, _, l_MAPE, _)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            if u_MAPE == 0 or l_MAPE == 0:
                f.write(f"{(u_MAPE+l_MAPE)},")
            else:
                f.write(f"{(u_MAPE+l_MAPE)/2.0},")
        f.write('\nME,')
        for idx, ((_, _, _, u_ME), (_, _, _, l_ME)) in enumerate(zip(interval_upper_detect_result[:, -1], interval_lower_detect_result[:, -1])):
            f.write(f"{np.max((u_ME, l_ME))},")
    
    for idx, data in enumerate(interval_upper_detect_result):
        if (interval_upper_detect_result[idx][-1][0] == 0 or interval_lower_detect_result[idx][-1][0] == 0):
            _MAE = interval_upper_detect_result[idx][-1][0] + interval_lower_detect_result[idx][-1][0]
        else:
            _MAE = (interval_upper_detect_result[idx][-1][0] + interval_lower_detect_result[idx][-1][0]) / 2.0

        if (interval_upper_detect_result[idx][-1][1] == 0 or interval_lower_detect_result[idx][-1][1] == 0):
            _RMSE = interval_upper_detect_result[idx][-1][1] + interval_lower_detect_result[idx][-1][1]
        else:
            _RMSE = (interval_upper_detect_result[idx][-1][1] + interval_lower_detect_result[idx][-1][1]) / 2.0

        if (interval_upper_detect_result[idx][-1][2] == 0 or interval_lower_detect_result[idx][-1][2] == 0):
            _MAPE = interval_upper_detect_result[idx][-1][2] + interval_lower_detect_result[idx][-1][2]
        else:
            _MAPE = (interval_upper_detect_result[idx][-1][2] + interval_lower_detect_result[idx][-1][2]) / 2.0

        _ME = np.max((interval_upper_detect_result[idx][-1][3], interval_lower_detect_result[idx][-1][3]))

        print(f"mean result :: MAE: {_MAE}, RMSE: {_RMSE}, MAPE: {_MAPE}, ME: {_ME}")

    # ------------------------- ç¹ªåœ– (åŸå§‹çµæœèˆ‡é æ¸¬) -------------------------
    plt.figure(figsize=(10, 12))
    plt.subplot(2, 1, 1)
    plt.title("HMamba")
    plt.xlabel('Process Data(X)')
    plt.ylabel('CoM')
    plt.plot(test_Y_2d.numpy(), marker=".", linestyle="-", color='blue', label='Ground Truth-CoM')
    plt.plot(mean_pred, marker="o", linestyle="-", color='orange', label='Model Predict-CoM')
    plt.axhline(y=70, color='green', label='Boundary Upper')
    plt.axhline(y=66, color='magenta', linestyle="--", label='Boundary Lower')
    plt.axhline(y=64, color='blue', linestyle="--", label='Sensitivity Upper')
    plt.axhline(y=60, color='red', label='Sensitivity Lower')
    plt.legend(frameon=False, bbox_to_anchor=(1, 1), loc=2, borderaxespad=0)

    plt.subplot(2, 1, 2)
    plt.title("HMamba")
    plt.xlabel('Process Data(X)')
    plt.ylabel('CoM')
    plt.plot(test_Y_2d.numpy(), marker="*", linestyle="", color='blue', label='Ground Truth-CoM')
    plt.plot(pred_test_net_combine.numpy(), marker="o", linestyle="-", color='orange', label='Model Predict-CoM')
    plt.axhline(y=70, color='green', label='Boundary Upper')
    plt.axhline(y=66, color='magenta', linestyle="--", label='Boundary Lower')
    plt.axhline(y=64, color='blue', linestyle="--", label='Sensitivity Upper')
    plt.axhline(y=60, color='red', label='Sensitivity Lower')
    plt.legend(frameon=False, bbox_to_anchor=(1, 1), loc=2, borderaxespad=0)
    plt.show()

    # ------------- ç†±åœ–ç¹ªè£½ -------------
    # é€™è£¡ç›´æ¥ä½¿ç”¨ model è¼¸å‡ºçš„ att_upper_map èˆ‡ att_lower_map
    try:
        attention = [pred_y[3], pred_y[4]]
    except NameError:
        print("è«‹ç¢ºèª att1 èˆ‡ att2 æ³¨æ„åŠ›åœ–å·²å®šç¾©ã€‚")
        attention = [np.zeros((test_Y_2d.shape[0], 1, 9, 9)), np.zeros((test_Y_2d.shape[0], 1, 9, 9))]
    maps = ['RoomTemp','PreformTemp','PreblowPressure','HighPressure','VentPressure',
            'OverallPressure','HighTankPressure','LowTankPressure','HandlePosition']
    result_path = "./result_heatmaps"  # å¯æ ¹æ“šéœ€æ±‚èª¿æ•´
    print("Heatmaps ç¹ªåœ–é–‹å§‹......")
    heatmap_drawer(attention=attention, gt_value=test_Y_2d.numpy(), pred_value=pred_test_net_combine.numpy(),
                   pred_cls=pred_test_net_cls, maps=maps, result_path=result_path)
    print("Heatmaps ç¹ªåœ–å®Œæˆ")
